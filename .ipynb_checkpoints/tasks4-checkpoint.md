---
layout: default
title: Tasks
---

# CLEF 2025 SimpleText: Tasks

---
## How to participate
In order to participate, you should sign up at the [CLEF](https://clef2025.clef-initiative.eu/index.php) website: [http://clef2025-labs-registration.dei.unipd.it/](http://clef2024-labs-registration.dei.unipd.it/). 

All team members should join the SimpleText mailing list:
[https://groups.google.com/g/simpletext](https://groups.google.com/g/simpletext). 

The data will be made available to all registered participants.

## Task 1: Text Simplification (simplify scientific text)
This task focuses on simplifying complex scientific texts to improve accessibility, particularly in the biomedical domain. It involves simplifying both sentence-level (Task 1.1) and document-level (Task 1.2) scientific texts. The latest approach for this task includes utilizing a new, extensive corpus created in 2024, specifically tailored to the biomedical field. Additionally, a third sub-task on text alignment is being considered, which requires aligning sentences between two related abstracts (such as source and reference abstracts or source and predictions). This alignment task allows for flexible matching, such as splitting or merging sentences, enhancing the simplification process for complex scientific documents.

### Data
The simplification dataset is built by re-aligning abstracts with their lay summaries on a large scale across sentence, paragraph, and document levels. This corpus is carefully structured to support sentence-level and document-level text simplification, with a special focus on the biomedical domain.

### Evaluation
The evaluation will combine automatic measures (such as SARI, BLEU, LENS, and BERTScore) with human assessments by translation students and professionals, adding a qualitative perspective. CLEF SimpleText has pioneered research in text simplification within information retrieval and NLP, aligning its evaluation approach with related fields like Scholarly Document Processing and the new TREC PABLA track, promoting a standardized methodology across similar tasks.

## Task 2: Identifying and explaining difficult concepts

This task addresses the detection and explanation of complex or "hallucinated" content generated by text simplification systems. Participants are provided with generated texts that may contain creatively exaggerated or unsupported information, with the goal of identifying and mitigating such content. With sources, predictions, and references all closely aligned in the same language, Task 2 explores how to maintain accuracy in generated content, studying how creative or unsupported variations can impact understanding.

### Description

Task 2.1 involves identifying instances of creative generation, particularly in system outputs at the abstract or document level. Participants are provided with realistic outputs from previous years and from known generative models, and they are asked to label sentences that are fully grounded in the source. This includes a focus on labeling sentences with and without access to the source, as well as those introducing significant new content. Task 2.2 shifts to preventing creative generation by ensuring grounding from the start, using source attribution to support claims in the text. Similar to Task 1, it requires participants to submit paired runsâone with and one without source attribution. A related third task, still under discussion, would ask participants to optimally align sentences and paragraphs in both source and output to ensure faithful source attribution.

### Data

Drawing on three years of SimpleText track data, this task uses an extensive collection of predictions and models known for generating spurious or unsupported content. For Task 2.1, data samples highlight unsupported sentences for training, while Task 2.2 adopts a paired run structure similar to that of Task 1, allowing for comparison between source-grounded and creatively generated text pairs.

### Evaluation

Task 2.1 will be assessed with standard classification metrics such as Precision, Recall, and F1 score, with additional token-level evaluations using Jaccard index. For Task 2.2, automatic evaluation measures will be complemented by human assessments, as in Task 1, with the paired structure enabling efficient evaluation of sentence- and phrase-level differences, potentially with tools like MT Unbabel.

## Task 3: LeaderBoardQA

Task 3, known as LeaderBoardQA, extends a pilot task from CLEF 2024 focused on information extraction in scientific documents. Participants are tasked with constructing a leaderboard-like output by using domain-specific question-answering (QA) to retrieve performance metrics on AI models. This task challenges participants to gather exact performance data and relevant metrics on specified AI models, or on models and benchmarks combined, directly from a scientific corpus. The approach encourages using advanced Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems, with both open-corpus and closed-book submission formats supported.

### Description

LeaderBoardQA requires extracting specific ânuggetsâ of information related to AI model performance from a full-text scientific corpus. These nuggets include exact details on model performance and relevant metrics, assessed across a range of benchmarks. Participants are expected to leverage LLMs and RAGs for accurate and reliable extraction, aiding in the construction of a comprehensive leaderboard for model performance. Building upon CLEF 2024's SimpleText SOTA task, this extension broadens its scope to evaluate the potential of domain-specific QA in generating benchmark datasets and improving AI performance transparency.

### Data and evaluation

For LeaderBoardQA, CLEF will reuse community-aligned training data from CLEF 2024 and enhance it with human-annotated gold standards to support rigorous evaluation. The task will follow QA evaluation protocols tailored for LLM and RAG systems, with results analysed against a comprehensive gold-standard corpus. This dataset will serve as a critical resource for benchmarking QA performance on scientific information extraction tasks, as detailed in the CLEF 2024 SimpleText Task 4 overview paper.

## Task 4: SimpleText 2024 Revisited

CLEF 2025 introduces a restructured SimpleText track, aimed at adapting to new objectives and participant interests. Task 4 serves as a transitional track, potentially continuing work from CLEF 2024 tasks based on demand. Specifically, it considers re-running Task 1 on Content Selection (abstract retrieval) and Task 2 on Complexity Spotting (identifying and explaining difficult concepts). The continuation of these tracks is contingent on active interest and input from participants and organisers, with discussions planned at the CLEF 2024 conference in Grenoble.

### Description

CLEF 2025 SimpleText is very different from the earlier years. In order to facilitate the transition to the new track setup, we consider continuing one of the other CLEF 2024 SimpleText tracks (Task 1 on Content Selection: abstract re- trieval, Task 2 on Complexity Spotting: identifying and explaining difficult concepts). We will only continue those activities at the request of, and with sufficient interest from, our active participants. We will discuss this with participants and the current team of organisers at CLEF 2024 in Grenoble

### Data and evaluation

For further reference on methodology and evaluation criteria, details are available in the LNCS track overview paper by Ermakova et al. (2024b), as well as in the CEUR task overview papers for CLEF 2024 SimpleText Task 1 (Sanjuan et al., 2024) and Task 2 (Di Nunzio et al., 2024).
